{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800b74d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ibm_watson\n",
      "  Downloading ibm-watson-8.0.0.tar.gz (398 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m398.3/398.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ibm-cloud-sdk-core==3.*,>=3.3.6\n",
      "  Downloading ibm-cloud-sdk-core-3.19.2.tar.gz (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /Applications/anaconda3/lib/python3.9/site-packages (from ibm_watson) (2.8.2)\n",
      "Collecting websocket-client>=1.1.0\n",
      "  Downloading websocket_client-1.7.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0,>=2.0 in /Applications/anaconda3/lib/python3.9/site-packages (from ibm_watson) (2.28.1)\n",
      "Collecting requests<3.0,>=2.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3.0.0,>=2.1.0\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting PyJWT<3.0.0,>=2.8.0\n",
      "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.5.3->ibm_watson) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.0->ibm_watson) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.0->ibm_watson) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/lib/python3.9/site-packages (from requests<3.0,>=2.0->ibm_watson) (2024.2.2)\n",
      "Building wheels for collected packages: ibm_watson, ibm-cloud-sdk-core\n",
      "  Building wheel for ibm_watson (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm_watson: filename=ibm_watson-8.0.0-py3-none-any.whl size=401037 sha256=ddf9c6587147d9072269fe4e1bae0a93a9fab70db4065d468efd9f0bc91474b3\n",
      "  Stored in directory: /Users/abc/Library/Caches/pip/wheels/48/ec/17/610bd906eff4ca0712390771eb4ab94dcdcc8906cf7655cd84\n",
      "  Building wheel for ibm-cloud-sdk-core (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-3.19.2-py3-none-any.whl size=98827 sha256=20e9c77d433a946b8c098f91b9880ba76f876a0b179a93a4446d139a5f8928ac\n",
      "  Stored in directory: /Users/abc/Library/Caches/pip/wheels/1b/51/d3/4f52df79153d51558239c31ef37578e6866c2c739f06ded562\n",
      "Successfully built ibm_watson ibm-cloud-sdk-core\n",
      "Installing collected packages: websocket-client, urllib3, PyJWT, requests, ibm-cloud-sdk-core, ibm_watson\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.11\n",
      "    Uninstalling urllib3-1.26.11:\n",
      "      Successfully uninstalled urllib3-1.26.11\n",
      "  Attempting uninstall: PyJWT\n",
      "    Found existing installation: PyJWT 2.4.0\n",
      "    Uninstalling PyJWT-2.4.0:\n",
      "      Successfully uninstalled PyJWT-2.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.11.1 requires ruamel-yaml, which is not installed.\n",
      "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires requests==2.28.1, but you have requests 2.31.0 which is incompatible.\n",
      "botocore 1.27.28 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyJWT-2.8.0 ibm-cloud-sdk-core-3.19.2 ibm_watson-8.0.0 requests-2.31.0 urllib3-2.2.1 websocket-client-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ibm_watson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31dd1269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textrazor\n",
      "  Downloading textrazor-1.4.1-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: textrazor\n",
      "Successfully installed textrazor-1.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textrazor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ece9b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"The sports and entertainment industry thrives on fan engagement, which is an essential component of its success. A strong and dedicated fan base not only generates revenue through ticket sales and merchandise but also adds value to the brand. However, with the proliferation of digital technology, fans now have an array of options and distractions at their disposal. Therefore, it has become imperative for organizations to utilize data and analytics to understand fan behavior and preferences. This is where fan engagement analytics comes in. By analyzing data and deriving actionable insights, organizations can create targeted strategies to enhance fan engagement and deliver unforgettable experiences that will keep fans coming back for more.Fan engagement analytics offers significant advantages to sports organizations. It allows them to gain a complete and thorough understanding of their fan behavior by analyzing data from various sources such as social media, ticket sales, and website interactions. The analysis of this information provides valuable insights into fan preferences, interests, and engagement levels, which can be used to tailor marketing efforts and communication strategies to best meet the specific needs and expectations of the fan base. This data-driven approach helps organizations to create a personalized fan experience and build a deeper connection with fans. Overall, fan engagement analytics is a powerful tool that enables organizations to improve fan engagement and satisfaction, leading to increased loyalty and revenue.Organizations can increase fan satisfaction and brand loyalty by understanding the demographics, interests, and behaviors of their fans. By doing so, they can deliver personalized content, offers, and experiences that make fans feel valued and appreciated. This level of customization not only enhances the likelihood of repeat engagement but also increases the chances of developing a long-term relationship with the fans.Fan engagement analytics can help optimize the game-day experience and increase fan satisfaction. By analyzing concession sales and social media feedback, organizations can identify popular choices and quickly address concerns to demonstrate their commitment to fan satisfaction. These insights can also help improve marketing campaigns, merchandise sales, and overall fan experience, leading to increased loyalty and revenue.Fan engagement analytics is a powerful tool to monitor and analyze social media conversations and sentiment. This data can be used to develop effective social media strategies, create compelling content, and engage in meaningful conversations with fans. Overall, it's a crucial component of any organization's social media strategy that provides valuable insights into fan behavior and preferences.Fan engagement analytics helps organizations justify investments in fan engagement initiatives. By tracking KPIs such as ticket sales, merchandise revenue, social media engagement, and fan satisfaction scores, organizations can assess the impact of their fan engagement strategies and make data-driven decisions for future initiatives.The Seattle Seahawks, a professional football team in the NFL, utilized data science techniques to enhance fan engagement and improve the stadium experience for their loyal supporters. The team gathered data from different sources, including ticketing systems, social media platforms, and in-stadium surveys. By analyzing this data, the Seahawks obtained insights into fan preferences, attendance patterns, and purchasing behavior. They utilized this information to personalize ticket pricing based on demand, offer targeted promotions and discounts, and enhance stadium operations.In the Indian Premier League (IPL), ESPNcricinfo uses Smart Stats during matches. Smart Stats offers insightful metrics and visualizations, such as the Expected Runs (ER) metric. This metric calculates a batsman's expected contribution to the game based on factors like runs scored, balls faced, and match context. During a high-scoring match, Smart Stats can reveal how a batsman's performance exceeds expectations. This provides fans with a deeper appreciation of their impact on the game beyond traditional statistics.of the noteworthy things in the English Premier League (EPL) is Manchester City's use of data-driven content on social media. The club employs advanced analytics to understand fan preferences and trends, which helps them create highly engaging content that resonates with their audience. For example, Manchester City may create visually stunning graphics highlighting key moments from a thrilling match, supported by insightful statistics and analysis. This approach captivates fans on social media, and reinforces Manchester City's reputation as a forward-thinking club that prioritizes fan engagement strategies.Kinexon Sport technology is being used by many famous sports teams and organizations across various sports, such as FC Bayern Munich, Dallas Mavericks (Basketball), and Red Bull Racing (Formula 1). Kinexon Sport utilizes ultra-wideband technology to track athletes' movements in real-time accurately. Though it is primarily used for optimizing team performance, it can also enhance fan engagement. Fans can benefit from live player tracking, interactive visualizations, augmented reality experiences, and engaging challenges based on the data collected. Overall, Kinexon Sport revolutionizes fan engagement by providing deeper insights and interactive experiences during live sporting events.Fan engagement analytics is a powerful tool that can help organizations in the sports and entertainment industry to increase fan engagement and create unforgettable experiences. By utilizing data and analytics, organizations can gain valuable insights into fan behavior, personalize their marketing efforts, improve the game-day experience, leverage social media platforms, and measure the ROI of their fan engagement initiatives.By adopting a data-driven approach, organizations can build a strong and dedicated fan base that generates revenue and enhances brand value. So, if you're looking to excel in the sports and entertainment industry, harnessing the power of fan engagement analytics should be your top priority.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57b5fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"hi myself as Kumar Singh and thank you for providing me the opportunity to introduce myself I am currently doing BTech in data Science from Heritage Institute of Technology and I am very much passionate about becoming a sports analyst in a new future I have done my schooling from a place called Children Day School Kalyani where I have completed 10th and my 12th with an aggregate of 9 and 8.3 respect and general knowledge olympiads which help me to get knowledge about different domains of science I am proficient in all three languages English Hindi and Bengali always enjoy the sense of teamwork and always been a team player and perform extremely well when I was in a team if I get an opportunity I am sure that my skills and knowledge is going to be the Asset to your project and ideas lineup which I am eagerly waiting to share with your team I am very excited to work with you and talented group of students thank you for your time and I am looking forward to hear from you\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39c0841f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge // 0.2051 // 1.277 // http://en.wikipedia.org/wiki/Knowledge\n",
      "Kumar Singh // 0 // 0.5 // \n",
      "Heritage Institute of Technology // 0 // 0.5 // \n",
      "Science // 0.2614 // 1.708 // http://en.wikipedia.org/wiki/Science\n",
      "Hindi // 0.1759 // 9.361 // http://en.wikipedia.org/wiki/Hindi\n",
      "Bengali language // 0.1853 // 5.56 // http://en.wikipedia.org/wiki/Bengali_language\n",
      "Teamwork // 0.06483 // 0.972 // http://en.wikipedia.org/wiki/Teamwork\n",
      "Knowledge // 0.2051 // 1.277 // http://en.wikipedia.org/wiki/Knowledge\n",
      "Knowledge // 0.2051 // 1.277 // http://en.wikipedia.org/wiki/Knowledge\n",
      "9 // 0 // 0.5 // \n",
      "8.29999999999999999931 // 0 // 0.5 // \n",
      "Bachelor of Technology // 0.1015 // 2.223 // http://en.wikipedia.org/wiki/Bachelor_of_Technology\n",
      "Data science // 0.2383 // 1.877 // http://en.wikipedia.org/wiki/Data_science\n",
      "Science // 0.2614 // 1.708 // http://en.wikipedia.org/wiki/Science\n",
      "Technology // 0.1903 // 1.229 // http://en.wikipedia.org/wiki/Technology\n",
      "10 // 0 // 0.5 // \n",
      "12 // 0 // 0.5 // \n"
     ]
    }
   ],
   "source": [
    "import textrazor\n",
    "\n",
    "textrazor.api_key = \"a2ee0328e56b8d53adf8c26f1791383018e8ea4ca9c7ad67691af8f8\"\n",
    "\n",
    "client = textrazor.TextRazor(extractors=[\"entities\"])\n",
    "response = client.analyze(text1)\n",
    "id1=[]\n",
    "rs=[]\n",
    "cs=[]\n",
    "ft=[]\n",
    "for entity in response.entities():\n",
    "    print(entity.id,\"//\",entity.relevance_score,\"//\",entity.confidence_score,\"//\",entity.wikipedia_link)\n",
    "    id1.append(entity.id)\n",
    "    rs.append(entity.relevance_score)\n",
    "    cs.append(entity.confidence_score)\n",
    "    ft.append(entity.wikipedia_link)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9476f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top three entries (id1, (cs, ft)):\n",
      "('Science', (0.2614, 'http://en.wikipedia.org/wiki/Science'))\n",
      "('Data science', (0.2383, 'http://en.wikipedia.org/wiki/Data_science'))\n",
      "('Knowledge', (0.2051, 'http://en.wikipedia.org/wiki/Knowledge'))\n",
      "('Technology', (0.1903, 'http://en.wikipedia.org/wiki/Technology'))\n",
      "('Bengali language', (0.1853, 'http://en.wikipedia.org/wiki/Bengali_language'))\n",
      "('Hindi', (0.1759, 'http://en.wikipedia.org/wiki/Hindi'))\n",
      "('Bachelor of Technology', (0.1015, 'http://en.wikipedia.org/wiki/Bachelor_of_Technology'))\n",
      "('Teamwork', (0.06483, 'http://en.wikipedia.org/wiki/Teamwork'))\n",
      "('Kumar Singh', (0, ''))\n",
      "('Heritage Institute of Technology', (0, ''))\n"
     ]
    }
   ],
   "source": [
    "unique_entities = {}\n",
    "for i, id_val in enumerate(id1):\n",
    "    if id_val not in unique_entities:\n",
    "        unique_entities[id_val] = (rs[i], ft[i])\n",
    "\n",
    "# Sort the dictionary by cs values in descending order\n",
    "sorted_entities = dict(sorted(unique_entities.items(), key=lambda x: x[1][0], reverse=True))\n",
    "id_m,cs_m,ft_m=[],[],[]\n",
    "# Print the top three entries\n",
    "top_three = list(sorted_entities.items())[:10]\n",
    "print(\"Top three entries (id1, (cs, ft)):\")\n",
    "for item in top_three:\n",
    "    print(item)\n",
    "    i,(m,n)=item\n",
    "    id_m.append(i)\n",
    "    cs_m.append(m)\n",
    "    ft_m.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d8b68d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e98f5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting text2emotion\n",
      "  Downloading text2emotion-0.0.5-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m804.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting emoji>=0.6.0\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.8/433.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk in /Applications/anaconda3/lib/python3.9/site-packages (from text2emotion) (3.7)\n",
      "Requirement already satisfied: joblib in /Applications/anaconda3/lib/python3.9/site-packages (from nltk->text2emotion) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.9/site-packages (from nltk->text2emotion) (4.64.1)\n",
      "Requirement already satisfied: click in /Applications/anaconda3/lib/python3.9/site-packages (from nltk->text2emotion) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Applications/anaconda3/lib/python3.9/site-packages (from nltk->text2emotion) (2022.7.9)\n",
      "Installing collected packages: emoji, text2emotion\n",
      "Successfully installed emoji-2.11.0 text2emotion-0.0.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install text2emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abf17f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/abc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/abc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/abc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "910d31d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /Applications/anaconda3/lib/python3.9/site-packages (2.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20f65799",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'emoji' has no attribute 'UNICODE_EMOJI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5w/d9ylxc2x30gggqgzmxv8y6440000gn/T/ipykernel_2272/2530165666.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrecognized_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hi myself as Kumar Singh and thank you for providing me the opportunity to introduce myself I am currently doing BTech in data Science from Heritage Institute of Technology and I am very much passionate about becoming a sports analyst in a new future I have done my schooling from a place called Children Day School Kalyani where I have completed 10th and my 12th with an aggregate of 9 and 8.3 respect and general knowledge olympiads which help me to get knowledge about different domains of science I am proficient in all three languages English Hindi and Bengali always enjoy the sense of teamwork and always been a team player and perform extremely well when I was in a team if I get an opportunity I am sure that my skills and knowledge is going to be the Asset to your project and ideas lineup which I am eagerly waiting to share with your team I am very excited to work with you and talented group of students thank you for your time and I am looking forward to hear from you\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mte\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_emotion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecognized_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36mget_emotion\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m   2714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2716\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2717\u001b[0m     \u001b[0memotion_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m     \u001b[0memotions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Happy\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Angry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Surprise\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sad\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fear\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36mcleaning\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m   2698\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2700\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memojis_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2701\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'http\\S+|www.\\S+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremoving_contradictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36memojis_extractor\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m   2567\u001b[0m                             \u001b[0;34m'Fear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m                             'Fear']}\n\u001b[0;32m-> 2569\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNICODE_EMOJI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2570\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/text2emotion/__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2567\u001b[0m                             \u001b[0;34m'Fear'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m                             'Fear']}\n\u001b[0;32m-> 2569\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNICODE_EMOJI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2570\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'emoji' has no attribute 'UNICODE_EMOJI'"
     ]
    }
   ],
   "source": [
    "recognized_text=\"hi myself as Kumar Singh and thank you for providing me the opportunity to introduce myself I am currently doing BTech in data Science from Heritage Institute of Technology and I am very much passionate about becoming a sports analyst in a new future I have done my schooling from a place called Children Day School Kalyani where I have completed 10th and my 12th with an aggregate of 9 and 8.3 respect and general knowledge olympiads which help me to get knowledge about different domains of science I am proficient in all three languages English Hindi and Bengali always enjoy the sense of teamwork and always been a team player and perform extremely well when I was in a team if I get an opportunity I am sure that my skills and knowledge is going to be the Asset to your project and ideas lineup which I am eagerly waiting to share with your team I am very excited to work with you and talented group of students thank you for your time and I am looking forward to hear from you\"\n",
    "te.get_emotion(recognized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420f79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nrclex\n",
      "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
      "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk>=3.8\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Applications/anaconda3/lib/python3.9/site-packages (from nltk>=3.8->textblob->nrclex) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /Applications/anaconda3/lib/python3.9/site-packages (from nltk>=3.8->textblob->nrclex) (4.64.1)\n",
      "Requirement already satisfied: joblib in /Applications/anaconda3/lib/python3.9/site-packages (from nltk>=3.8->textblob->nrclex) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Applications/anaconda3/lib/python3.9/site-packages (from nltk>=3.8->textblob->nrclex) (2022.7.9)\n",
      "Building wheels for collected packages: nrclex\n",
      "  Building wheel for nrclex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nrclex: filename=NRCLex-3.0.0-py3-none-any.whl size=43310 sha256=72079c478d43afffbca907205d71a5cfde9e0956f81ab578d203e024036e5100\n",
      "  Stored in directory: /Users/abc/Library/Caches/pip/wheels/68/c4/f2/c390dd3eac398fdf45f7a01c6516bc53fa7a9ab59c7d2ff518\n",
      "Successfully built nrclex\n",
      "Installing collected packages: nltk, textblob, nrclex\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.7\n",
      "    Uninstalling nltk-3.7:\n",
      "      Successfully uninstalled nltk-3.7\n",
      "Successfully installed nltk-3.8.1 nrclex-3.0.0 textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nrclex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095e32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nrclex import NRCLex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788d2f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'anticipation': 10,\n",
       " 'joy': 6,\n",
       " 'positive': 16,\n",
       " 'trust': 12,\n",
       " 'negative': 1,\n",
       " 'surprise': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"hi myself as Kumar Singh and thank you for providing me the opportunity to introduce myself I am currently doing BTech in data Science from Heritage Institute of Technology and I am very much passionate about becoming a sports analyst in a new future I have done my schooling from a place called Children Day School Kalyani where I have completed 10th and my 12th with an aggregate of 9 and 8.3 respect and general knowledge olympiads which help me to get knowledge about different domains of science I am proficient in all three languages English Hindi and Bengali always enjoy the sense of teamwork and always been a team player and perform extremely well when I was in a team if I get an opportunity I am sure that my skills and knowledge is going to be the Asset to your project and ideas lineup which I am eagerly waiting to share with your team I am very excited to work with you and talented group of students thank you for your time and I am looking forward to hear from you\"\n",
    "emotion=NRCLex(text)\n",
    "emotion.raw_emotion_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
